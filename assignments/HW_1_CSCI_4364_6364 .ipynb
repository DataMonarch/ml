{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PL40ctmMcim"
      },
      "source": [
        "# Homework 1 CSCI 4364/6364 Machine Learning\n",
        "\n",
        "## **ML Environment and Fundamentals**\n",
        "\n",
        "**Due Date: 9/12/2023, 23:59 ET**\n",
        "\n",
        "**Purpose**\n",
        "This first homework assignment is to familiarize you with the libraries commonly used in machine learning applications, and apply them to some fundamental concepts, such cross-validation, and hyperparameter tuning, used frequently in model training and evaluation.\n",
        "\n",
        "This homework touches on many practical considerations when training and evaluating machine learning models. It’s more structured than future assignments, which will be progressively less defined as you gain experience in applying your knowledge in machine learning.\n",
        "\n",
        "Submission Instructions\n",
        "This assignment will be done entirely in this Colaboratory notebook, and you will submit your notebook via GWU blackboard. Please embed your code in code blocks and add in comments into the text blocks.\n",
        "\n",
        "Grading on the notebook\n",
        "This notebook is worth 5% of the semester grade, where 3% is completion and full functionality, and 2% is based on comments and descriptions, and well-written and commented Python code, based on the coding standards. The notebook should be fully explained and work in its entirety when you submit it.\n",
        "\n",
        "**Important:** Please ensure that you save output when submitting the colab. You can verift this by going to Edit >> Settings and making sure the checkbox next to \"Omit code output when saving notebook\" is **NOT** checked.\n",
        "\n",
        "Coding Standards\n",
        "Throughout this course, we will use Google’s Python Style Guide (https://google.github.io/styleguide/pyguide.html) as the coding standard for homework and project submission. A big part of machine learning in industry is applying good programming practices.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2iRXG9B5hPK"
      },
      "source": [
        "**Name:** Toghrul Tahirov\n",
        "\n",
        "**GW ID:** G47609664"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "DnXm2ekDUw3P"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "##########################################################\n",
        "# Always include all imports at the first executable cell.\n",
        "##########################################################\n",
        "from abc import ABC, abstractmethod # Abstract Base Classes for Python\n",
        "import tensorflow as tf\n",
        "# TODO import Tensorflow datasets\n",
        "import tensorflow_datasets as tfds\n",
        "import pandas as pd # Pandas dataframe libaries\n",
        "import numpy as np # Numpy numerical computation library\n",
        "from sklearn.linear_model import LogisticRegression # Linear Classifier\n",
        "from sklearn import metrics # Used to compute metrics, such as the Area Under the Curve (AUC)\n",
        "import matplotlib.pyplot as plt # Plotting library.\n",
        "from typing import List, Tuple, Mapping # Common types for Python type definitions\n",
        "from tensorboard.plugins.hparams import api as hp # Library for Hyperparameter Tuning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jl-iRSxhNY2m"
      },
      "source": [
        "# 1 Datasets\n",
        "There are many datasets available in open source to try out machine learning algorithms. Besides being accessible and easy to use, many serve as well-known reference datasets that you can use to demonstrate general applicability in scientific publications.\n",
        "\n",
        "In this assignment we’ll gain experience with  the [TensorFlow Data Set (TFDS)](https://www.tensorflow.org/datasets) by using the “[German Credit Numeric](https://www.tensorflow.org/datasets/catalog/german_credit_numeric)” data set.\n",
        "\n",
        "You can use open source data sets or source your own non-proprietary data set for your semester project.\n",
        "\n",
        "You can gain additional practice using TFDS by reviewing and executing the [colab](https://colab.research.google.com/github/tensorflow/datasets/blob/master/docs/overview.ipynb#scrollTo=Y0iVVStvk0oI).\n",
        "\n",
        "Here, we simply load the German Credit Numeric dataset and convert the whole dataset to a Pandas dataframe. With larger data sets you may find it more efficient to load the data set and use `take(N)` to retrieve a smaller subsample incrementally.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "FBVlzaqTMLEi"
      },
      "outputs": [],
      "source": [
        "#@title Download the data set from TensorFlow Datasets (TFDS)\n",
        "\n",
        "# Source https://www.tensorflow.org/datasets/catalog/german_credit_numeric\n",
        "\n",
        "ds, ds_info = tfds.load(\n",
        "    'german_credit_numeric', split='train',\n",
        "    shuffle_files=True, with_info = True)\n",
        "df = tfds.as_dataframe(ds.take(1000), ds_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explorative Data Analysis\n",
        "\n",
        "### Quick look at the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tfds.core.DatasetInfo(\n",
              "    name='german_credit_numeric',\n",
              "    full_name='german_credit_numeric/1.0.0',\n",
              "    description=\"\"\"\n",
              "    This dataset classifies people described by a set of attributes as good or bad\n",
              "    credit risks. The version here is the \"numeric\" variant where categorical and\n",
              "    ordered categorical attributes have been encoded as indicator and integer\n",
              "    quantities respectively.\n",
              "    \"\"\",\n",
              "    homepage='https://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)',\n",
              "    data_path='C:\\\\Users\\\\togru\\\\tensorflow_datasets\\\\german_credit_numeric\\\\1.0.0',\n",
              "    file_format=tfrecord,\n",
              "    download_size=99.61 KiB,\n",
              "    dataset_size=58.61 KiB,\n",
              "    features=FeaturesDict({\n",
              "        'features': Tensor(shape=(24,), dtype=int32),\n",
              "        'label': ClassLabel(shape=(), dtype=int64, num_classes=2),\n",
              "    }),\n",
              "    supervised_keys=('features', 'label'),\n",
              "    disable_shuffling=False,\n",
              "    splits={\n",
              "        'train': <SplitInfo num_examples=1000, num_shards=1>,\n",
              "    },\n",
              "    citation=\"\"\"@misc{Dua:2019 ,\n",
              "    author = \"Dua, Dheeru and Graff, Casey\",\n",
              "    year = \"2017\",\n",
              "    title = \"{UCI} Machine Learning Repository\",\n",
              "    url = \"http://archive.ics.uci.edu/ml\",\n",
              "    institution = \"University of California, Irvine, School of Information and Computer Sciences\"\n",
              "    }\"\"\",\n",
              ")"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'tensorflow_datasets.core.as_dataframe.StyledDataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   features  1000 non-null   object\n",
            " 1   label     1000 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 15.8+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df['features']\n",
        "y = df['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((1000,), (1000,))"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(pandas.core.series.Series, pandas.core.series.Series)"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(X), type(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(dtype('O'), dtype('int64'))"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# print the datatypes of the samples in X and y\n",
        "X.dtype, y.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((24,), dtype('int32'))"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check out the dimensions of the samples in X and their datatypes\n",
        "X[0].shape, X[0].dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "label\n",
              "1    700\n",
              "0    300\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check the unique values in y\n",
        "y.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jw5hFCgGRAt"
      },
      "source": [
        "## Question 1.1\n",
        "Describe the dataset you just downloaded. (a) How many examples and what types of features are there? (b) What do the labels represent?\n",
        "\n",
        "## Answer\n",
        "\n",
        "From the outputs of the above code block, we can see that the dataset has 1000 examples in terms of both the independent and dependent variables and both pandas.Series objects. The featuers data was extracted under the variable `X` and the labels were extracted under the variable `y`. Each row of the features dataset consists of a numpy array (np.ndarray) of **24** elements, thus the dataset is noted to contain 'object' type data. The 24 elements are all of type `int32`. These variables are the features of the dataset and each feature represents a certain characteristic of the credit applicant. Based on the explanation of the dataset, the features apparently include categorical features encoded as integers in addition to numerical features.\n",
        "\n",
        "The labels are of type `int64` and each label represents the credit risk of the person in terms of good or bad. This is lucid from the info given in the tf dataset object and we can confirm this by looking at the output of the above code block where we can see that the labels are either 0 or 1. Most probably, 0 represents bad credit risk and 1 represents good credit risk judged by the distribution of the labels since we 70% of the labels are 1 and 30% are 0.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Wrangling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldCWm8U3Q_m0"
      },
      "source": [
        "Unfortunately, the Pandas dataframe `df` isn’t exactly in the right format. In this step, convert the raw data frame into a concatenated numpy array $[X|y]$, where $X$ is an array with $M$ example rows $\\times N$ feature columns and $y$ is the class label array with shape  $M \\times 1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Separate features and labels datasets have already been generated in the above steps, however the shape of the features dataset is not in the right format. The shape of the features dataset is (1000, 1) and we need to convert it to (1000, 24). We can do this by using the `np.vstack()` function which will vertically stack the nested np.array objects and result in the desired shape. \n",
        "\n",
        "We will also need to reshape the labels dataset to (1000, 1) as it is currently in the shape of (1000,). We can do this by using the `np.reshape()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "H3tOfXCsq2iH"
      },
      "outputs": [],
      "source": [
        "#@title Convert the dataset into an example array and a label array dataframe\n",
        "# this step has already been done in the above section\n",
        "\n",
        "# In this step, convert df into an numpy array called Xy.\n",
        "X = X.values\n",
        "y = y.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(numpy.ndarray, numpy.ndarray)"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# let's check the type of X and y to make sure they are numpy arrays\n",
        "type(X), type(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see both the features and labels datasets are now numpy arrays as desired"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's create a copy of the features dataset for future convenience\n",
        "X_cpy = X.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll now format both datasets into the desired shape through the methods described above. We'll then concatenate the two datasets into one dataset to get the desired shape of $[X|y]$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X.shape = (1000, 24), X.dtype = int32\n",
            "y.shape = (1000, 1), y.dtype = int64\n"
          ]
        }
      ],
      "source": [
        "# convert the np.ndarray of np.ndarray with shape Mx1 into a np.ndarray of int32 with shape MxN\n",
        "X = np.vstack(X_cpy)\n",
        "\n",
        "# convert the labels into a np.ndarray with shape Mx1\n",
        "y = y.reshape(-1,1)\n",
        "\n",
        "# let's confirm the shapes of X and y and the datatypes of their elements\n",
        "print(f\"X.shape = {X.shape}, X.dtype = {X.dtype}\")\n",
        "print(f\"y.shape = {y.shape}, y.dtype = {y.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the labels are of type int64, let's convert them to int32 for compatibility with the features dataset. This will eliminate the need for implicit type conversion when we concatenate the two datasets or when we use them in the future in our models. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "y = y.astype(\"int32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X.shape = (1000, 24), X.dtype = int32\n",
            "y.shape = (1000, 1), y.dtype = int32\n"
          ]
        }
      ],
      "source": [
        "# let's confirm the shapes of X and y and the datatypes of their elements\n",
        "print(f\"X.shape = {X.shape}, X.dtype = {X.dtype}\")\n",
        "print(f\"y.shape = {y.shape}, y.dtype = {y.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, both the features and labels datasets are of type int32 and in the desired shapes. We can now concatenate them into one dataset. I'll use the np.hstack() function to horizontally stack the two datasets which is a shorthand for concatenating the two datasets along axis 1. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ds_concat.shape = (1000, 25)\n"
          ]
        }
      ],
      "source": [
        "# converting the features and labels datasets into a single numpy array\n",
        "ds_concat = np.hstack((X,y))\n",
        "\n",
        "# confirming the shapes of the concatenated array\n",
        "print(f\"ds_concat.shape = {ds_concat.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIcHZSWnXBBO"
      },
      "source": [
        "Sequential bias occurs when patterns contained in the dataset may shift and results are order dependent. Imagine this data set was ordered by applicants from different geographic regions with different cultural practices and habits. If you split the data into a training set and a test set without countering the biases, you may be evaluating the model on applicants with different patterns the model has never seen before. Perhaps worse, suppose you deployed the model with the hidden bias into the real world, and the model worked better for applicants from Hamburg than applicants from Bavaria, causing an unfair bias.\n",
        "\n",
        "See also [Bias and unfairness in machine learning models: A systematic literature review](https://arxiv.org/pdf/2202.08176.pdf) for greater detail.\n",
        "\n",
        "A simple and effective technique for mitigating sequential bias is to randomize the data set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-6TpS_wP3iCP"
      },
      "outputs": [],
      "source": [
        "#@title Set a random seed prior to shuffle\n",
        "np.random.seed(seed = 773)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8afdjJt9XPZ7"
      },
      "source": [
        "## Question 1.2\n",
        "What's the reason we seed a random number generator before shuffeling the data set?\n",
        "\n",
        "\n",
        "## Answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yIUQUN2tHKUF"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Xy' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\togru\\ADA\\ml\\assignments\\HW_1_CSCI_4364_6364 .ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/togru/ADA/ml/assignments/HW_1_CSCI_4364_6364%20.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#@title Randomize the order of the dataset\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/togru/ADA/ml/assignments/HW_1_CSCI_4364_6364%20.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mshuffle(Xy)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'Xy' is not defined"
          ]
        }
      ],
      "source": [
        "#@title Randomize the order of the dataset\n",
        "np.random.shuffle(Xy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obH6goj1HkSs"
      },
      "source": [
        "# 2 Train a basic Linear Classifier on the data\n",
        "\n",
        "Next, we’ll train a Linear Classifier called [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) using Scikit-Learn. Logistic Regression is basically a linear regression with a softmax output, but don’t worry about the details yet - we’ll cover them in the coming weeks. Basically, we want to try out a linear classifier and see how it works “right out of the box.” This will serve as a baseline against which we’ll compare other models against."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvOTsLf5mGVD"
      },
      "source": [
        "Before we train our Logistic Regression model we’ll split the data into training and testing sets, training on the first 80% and testing on the last 20%. (We can do this since we’ve already shuffled the data and removed any sequential bias.)\n",
        "\n",
        "If we test on the training data, we’re only evaluating how well the model was able to memorize the training data and not how well the model is able to generalize on data it hasn’t seen before. For that reason we’ll evaluate the performance on a 20% test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPEGjHGBuNbX"
      },
      "outputs": [],
      "source": [
        "#@title Split into training (80%) and test (20%)\n",
        "split_ind = int(Xy.shape[0] * 0.8)\n",
        "# Training set\n",
        "# TODO variable names X_test, y_test\n",
        "# Test set\n",
        "# TODO variable names X_val, y_val\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "966owwOAnPV3"
      },
      "source": [
        "In the cell below, instantiate LogisticRegresion, train the model with the training data by executing `fit(X_train, y_train)`, and predict the class probabilities on the test set by calling `predict_proba(X_val)`.\n",
        "\n",
        "You may have to try different parameters to make it work without errors and warnings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yT1wCdci6Oev"
      },
      "outputs": [],
      "source": [
        "#@title Using Scikit-Learn, train a logistic regression model on the training set and test on the test set.\n",
        "# Construct and fit the LogtisticRegression model with the training data.\n",
        "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
        "# Predict the results on the test set (X_val).\n",
        "logistic_regression_preds = clf.predict_proba(X_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFM-99wn8Q0W"
      },
      "source": [
        "##Question 2.1:\n",
        "\n",
        "1. Were there any problems running logistic regression on the data set? If so what changes did you have to make and why?\n",
        "\n",
        "##Answer:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrHYSqA5IHBd"
      },
      "source": [
        "# 3 Evaluating the Binary Classifier\n",
        "\n",
        "So now that we have trained our first linear binary classifier, we need a way of evaluating its performance. Starting with the basics, for each prediction there are four possible outcomes:\n",
        "*  **True positives**: True class 1 is predicted to be class 1 (bad credit)\n",
        "*  **True negatives**: True class 0 is predicted to be class 0 (good credit)\n",
        "*  **False positives**: True class 0 is incorrectly predicted to be class 0\n",
        "*  **False negatives**: True class 1 is predicted to be class 0\n",
        "\n",
        "These counts can be converted into rates by dividing by the sample size (i.e., true positive rate, false positive rate, etc.). For more information about classification metrics, please refer to [Classification: True vs. False and Positive vs. Negative](https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative).\n",
        "\n",
        "We often use [**accuracy**](https://developers.google.com/machine-learning/crash-course/classification/accuracy), [**precision** and **recall**](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall) as measures of performance. See the links for a detailed discussion about these topics.  \n",
        "\n",
        "However, precision and recall have a limitation, because they impose a decision threshold to be calculated. In our problem, we may choose a decision threshold of 0.5, and classify all examples with scores above the threshold as class 1 and others as class 0. However, in some implementations, we can tolerate higher false positive errors than false negative errors, or vice versa.\n",
        "\n",
        "We need a measure of performance that does not require us to set a decision threshold so that we can compare the overall performance of one classifier against another. For this reason the [**ROC-AUC**](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)  is often used for binary classification evaluation.\n",
        "\n",
        "So the ROC-AUC (we’ll refer to it as the AUC for simplicity) is the integral of the True Positive Rate vs. False Positive Rate curves, and perfect classification scores a flat 1.0, and random guessing is 0.5. Any point on the ROC-AUC is an operating point with its own decision threshold, precision and recall.\n",
        "\n",
        "Scikit-Learn offers many methods in the [metrics library](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)bold text to calculate performance metrics, including the ROC-AUC. It’s done by first calling `roc_auc()` to compute the false positive and true positive rates, and then calling `auc()` to compute to integrate the curve and return the score.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISbTkOAt-mnm"
      },
      "outputs": [],
      "source": [
        "#@title Next, let's evaluate the prediction performance in AUC\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html\n",
        "\n",
        "# Pass the test labels, and the predicted labels to get the TPR and FPR.\n",
        "# TODO\n",
        "# Using the fpr, tpr compute the AUC.\n",
        "# TODO compuet roc_auc\n",
        "print('The ROC-AUC is %3.3f' %roc_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxSNI7zb5VI2"
      },
      "source": [
        "## Question 3.1:\n",
        "Consider the following scenario:\n",
        "\n",
        "*A deadly, but curable, medical condition afflicts .01% of the population. An ML model uses symptoms as features and predicts this affliction with an accuracy of 99.99%.*\n",
        "\n",
        "Is there any problem with using accuracy as a performance measure in this scenario?\n",
        "\n",
        "##Answer\n",
        "\n",
        "\n",
        "##Question 3.2:\n",
        "Your colleague asks for assistance in troubleshooting an ML performance problem, reporting an AUC of nearly 0. What could be the problem and how would you fix it?\n",
        "\n",
        "##Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsxTlrue5s8D"
      },
      "source": [
        "It's instructive to plot the ROC curve and display the AUC score. In the code below, we use [Matplotlib](https://matplotlib.org/3.1.0/index.html) to create a visually appealing plot.\n",
        "\n",
        "Note from the curve how the linear model (orange curve) performs much better than random guessing (diagonal blue dashed line). However, there's still plenty room for improvement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQuWCXPY_2E3"
      },
      "outputs": [],
      "source": [
        "#@title Utility to plot the ROC-AUC curve and score.\n",
        "def plot_auc(fpr: np.array, tpr: np.array) -> None:\n",
        "  \"\"\"Plots the ROC characteristic and the AUC Score\n",
        "\n",
        "  Based on https://scikit-learn.org/stable/auto_examples/model_selection/\n",
        "  plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py\n",
        "\n",
        "  Args:\n",
        "    fpr: False positive rate\n",
        "    tpr: True positive rate\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(10,10))\n",
        "  roc_auc = metrics.auc(fpr, tpr)\n",
        "  lw = 2\n",
        "  ax.plot(\n",
        "      fpr,\n",
        "      tpr,\n",
        "      color=\"darkorange\",\n",
        "      lw=lw,\n",
        "      label=\"ROC curve (area = %0.2f)\" % roc_auc,\n",
        "  )\n",
        "  ax.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
        "  ax.set_xlim([0.0, 1.0])\n",
        "  ax.set_ylim([0.0, 1.0])\n",
        "  plt.xlabel(\"False Positive Rate\")\n",
        "  plt.ylabel(\"True Positive Rate\")\n",
        "  plt.title(\"Receiver operating characteristic\")\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  plt.show()\n",
        "\n",
        "plot_auc(fpr, tpr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjD_ID4hI2sr"
      },
      "source": [
        "# 4 Common class definitions for comparing ML Algorithms\n",
        "\n",
        "Now that we have a reasonable baseline performance, it’s reasonable to ask whether alternative learning algorithms might do a better job. However, some  algorithms may be in different ML libraries, like Keras, PyTorch, TensorFlow, or stand-alone libraries posted in Github. Some libraries might not expose the same methods as `fit()` or `predictproba()`, so it’s useful to define an base class and use standard object-oriented programming and extend the base class with a specific implementation. This way we can run the same train/eval process for various different learning algorithms in a consistent manner.\n",
        "\n",
        "Below, we defined abstract `BaseLearningAlgorithm` that defines `train()`, `predict()` methods, and a name property for labeling the plots. Then, we create a very simple wrapper class `LogisticRegressionLearningAlgorithm`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0WJ6C6uFTJu"
      },
      "outputs": [],
      "source": [
        "class BaseLearningAlgorithm(ABC):\n",
        "  \"\"\"Base class for a Supervised Learning Algorithm.\"\"\"\n",
        "\n",
        "  @abstractmethod\n",
        "  def train(self, X_train:np.array, y_train: np.array) -> None:\n",
        "    \"\"\"Trains a model from labels y and examples X.\"\"\"\n",
        "\n",
        "  @abstractmethod\n",
        "  def predict(self, X_test: np.array) -> np.array:\n",
        "    \"\"\"Predicts on an unlabeled sample, X.\"\"\"\n",
        "\n",
        "  @property\n",
        "  @abstractmethod\n",
        "  def name(self) -> str:\n",
        "    \"\"\"Returns the name of the algorithm.\"\"\"\n",
        "\n",
        "\n",
        "class LogisticRegressionLearningAlgorithm:\n",
        "  \"\"\"Logistic Regression implementation of BaseLearningAlgorithm.\"\"\"\n",
        "\n",
        "  def __init__(self, max_iter):\n",
        "    self._model = LogisticRegression(random_state=0, max_iter = max_iter)\n",
        "\n",
        "  def train(self, X_train:np.array, y_train: np.array) -> None:\n",
        "    \"\"\"Trains a model from labels y and examples X.\"\"\"\n",
        "    # y_train = y_train.reshape(y_train.shape[0],)\n",
        "    self._model.fit(X_train, y_train)\n",
        "\n",
        "  def predict(self, X_test: np.array) -> np.array:\n",
        "    \"\"\"Predicts on an unlabeled sample, X.\"\"\"\n",
        "    return self._model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "  @property\n",
        "  def name(self) -> str:\n",
        "    return \"Logistic Regression\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVTT1_wB8cSV"
      },
      "source": [
        "# 5 Cross-fold Validation\n",
        "Even though we’re in the era of ‘Big Data’, high-quality labeled data is ironically still hard to get in large quantities. With only 1,000 labeled data points in our data set, we have only 800 for training and 200 for test. So, we want to maximize the use of our data without accidentally overfitting.\n",
        "\n",
        "Our next model will be a dense neural network with many different settings, or hyperparameters. Some hyperparameter settings yield better performance than others, which we’ll explore in the following sections.\n",
        "\n",
        "Suppose we choose hyperparameters, train our model on the 80%, test our model on 20%, and adapt the hyperparameters, and repeat this process maximizing the performance. There is still a danger of overfitting to the test set since we’re selecting hyperparameters that fit the test set best. If we deploy this model on a new dataset, we may observe degraded performance because the model was tuned to the test set.\n",
        "\n",
        "To prevent overfitting to the test set, we must perform our hyperparameter search on the 80% training set using cross-fold validation, and then publish final results using the 20% test set.\n",
        "\n",
        "In cross-fold validation, you take the original 80% training set, and split it into $k$ equally sized splits, train $k$ models on $k-1$ splits, test on the one split not used in the not in the $k-1$ splits used in training. Repeat the process $k$-times with $k$ results (here, AUC scores, TPRs, and FPRs). Then for the next iteration, choose a different set of hyperparameters, and repeat the process. Train a model using the best hyperparameters, and finally report the final results on the original 20% test data set that was not used while iterating on hyperparameters. For a detailed explanation, please see Algorithm 5.1, p120 in Goodfellow, and  [Cross-validation: evaluating estimator performance](https://scikit-learn.org/stable/modules/cross_validation.html).\n",
        "\n",
        "While you can choose any $k > 2$, the most common choices of $k$ are 3, 5, and 10. Here, we’ll specify `K_FOLDS=5`.\n",
        "\n",
        "While Scikit-Learn provides utilities for cross-fold validation, it’s instructive to write our own solution using Algorithm 5.1 as a reference and the BaseLearningAlgorithm we created above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxMWCG1fMSH1"
      },
      "outputs": [],
      "source": [
        "K_FOLDS = 5 # % splits of 80%/20% each"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "m0IbFBNR3lFr"
      },
      "outputs": [],
      "source": [
        "#@title Perform cross-validation train and eval\n",
        "Examples = np.array # [M examples, N features]\n",
        "Labels  = np.array # [M Labels]\n",
        "Results = Mapping[str, List[float]] # keys = roc_aus, fpr, tpr\n",
        "Fold = Tuple[Examples, Labels] # A single fold consists of examples and labels.\n",
        "\n",
        "def split_folds(X: np.array, y: np.array, k_folds: int) ->  List[Fold]:\n",
        "  \"\"\"Splits the data into consecutive folds.\n",
        "\n",
        "  Args:\n",
        "    X: n-d Array of examples\n",
        "    y: 1-d array of labels\n",
        "    k_folds: number of folds\n",
        "\n",
        "  Returns:\n",
        "    List of tuples of (X_i, y_i) i <= n_folds\n",
        "  \"\"\"\n",
        "  if X.shape[0] != y.shape[0]:\n",
        "    raise ValueError('The number of examples didn\\'t match match (|X| = %d and |y| = %d).', X.shape[0], y.shape[0])\n",
        "  split_size = int(X.shape[0]/k_folds)\n",
        "  folds = []\n",
        "  for fold_id in range(k_folds):\n",
        "    start_ix  = fold_id * split_size\n",
        "    end_ix = (fold_id + 1)* split_size\n",
        "    folds.append((X[start_ix: end_ix, :], y[start_ix: end_ix] ))\n",
        "\n",
        "  return folds\n",
        "\n",
        "\n",
        "def train_eval_xval(X: np.array, y: np.array,  k_folds: int, learning_algorithm: BaseLearningAlgorithm) -> Results:\n",
        "  \"\"\"Executes Cross-fold Train and Evaluation.\n",
        "\n",
        "  See Algorithm 5.1 (p. 120) in Goodfellow for implementation details. Instead\n",
        "  of minimizieng Loss Function, we'll be maximizing the AUC, which is already\n",
        "  a part of the BaseLearningAlgorithm predict() method.\n",
        "\n",
        "  Args:\n",
        "    X: An M x N array of examples\n",
        "    y: An N x 1 array of binary class labels\n",
        "    k_folds: The number of folds for cross-fold validation.\n",
        "    learning_algorithm: The algorithm being evaluated.\n",
        "\n",
        "  Returns:\n",
        "    A Results distcionary with True Pos Rate, False Pos Rate, and ROC-AUC.\n",
        "  \"\"\"\n",
        "\n",
        "  folds = split_folds(X, y, k_folds)\n",
        "\n",
        "  results_fpr = []\n",
        "  results_tpr = []\n",
        "  results_roc_auc = []\n",
        "\n",
        "  for fold_id in range(k_folds):\n",
        "    fold_indexes = np.arange(k_folds) # Creates a list of [0,...,k-1]\n",
        "\n",
        "    test_fold_index = fold_id # The test set is the fold id.\n",
        "\n",
        "    # Get the list of training indexes be removing the test index.\n",
        "    training_fold_indexes = np.delete(fold_indexes, fold_id)\n",
        "\n",
        "    # Create training set of examples and labels excluding test fold.\n",
        "    X_train = np.vstack([folds[k][0] for k in training_fold_indexes])\n",
        "    y_train = np.hstack([folds[k][1] for k in training_fold_indexes])\n",
        "\n",
        "    # Create a test set with labels for the test fold.\n",
        "    X_test = folds[test_fold_index][0]\n",
        "    y_test = folds[test_fold_index][1]\n",
        "\n",
        "    # Train the learning algorithm on the training set with labels.\n",
        "    learning_algorithm.train(X_train, y_train)\n",
        "\n",
        "    # Using the test set get the predictions.\n",
        "    predictions = learning_algorithm.predict(X_test)\n",
        "\n",
        "    # Compute the false positive and true positive rates.\n",
        "    fpr, tpr, _ = metrics.roc_curve(y_test, predictions)\n",
        "    # Get the ROC Area under the curve, by integrating the fpr/tpr curve.\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "    # Place the results into lists of length k.\n",
        "    results_fpr.append(fpr)\n",
        "    results_tpr.append(tpr)\n",
        "    results_roc_auc.append(roc_auc)\n",
        "\n",
        "  return {'roc_auc': results_roc_auc, 'tpr': results_tpr, 'fpr': results_fpr}\n",
        "\n",
        "# TODO: check max_iter...\n",
        "learning_algorithm = LogisticRegressionLearningAlgorithm(max_iter= 100)\n",
        "results = train_eval_xval(X_train, y_train, K_FOLDS, learning_algorithm)\n",
        "print(\"%s average auc = %3.3f\" %(learning_algorithm.name, np.mean(results['roc_auc'])))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4EIwoNiPkyE"
      },
      "source": [
        "## Enhance the ROC/AUC Plot to display $k$ results\n",
        "Since we are now getting $k$ ROC curves and $k$ AUC scores for each hyperparameter setting, we’ll want to enhance the plot to show the mean as a line and standard deviation of the folds as a shaded region around the lines. You can adapt the example provided by [Scikit-Learn]( https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRYrG-IcIelb"
      },
      "outputs": [],
      "source": [
        "#@title Create AUC Plots for Cross-Fold Validation\n",
        "def plot_auc_xfold(results, algorithm_name):\n",
        "  \"\"\"Plots ROC characteristics for X-fold validation runs.\n",
        "\n",
        "  See Scikit-Learn implementation as a basis:\n",
        "  https://scikit-learn.org/stable/auto_examples/model_selection/\n",
        "  plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py\n",
        "\n",
        "  Args:\n",
        "    results: Dict of tprs, fprs, and auc from k-folds.\n",
        "    algorithm_name: Name for labeling the chart.\n",
        "\n",
        "  \"\"\"\n",
        "  k_folds = len(results['tpr'])\n",
        "  fig, ax = plt.subplots(figsize=(10,10))\n",
        "  fprs = [results['fpr'][i] for i in range(k_folds)]\n",
        "  tprs = [results['tpr'][i] for i in range(k_folds)]\n",
        "\n",
        "  # Since the fprs and tprs don't necessarily have the same lengths,\n",
        "  # we need to apply linear interpolation with 100 equal-sized steps between\n",
        "  # 0 and 1.\n",
        "  mean_fpr = np.linspace(0, 1, 100)\n",
        "  interp_tprs = []\n",
        "  for i in range(k_folds):\n",
        "    interp_tpr = np.interp(mean_fpr, fprs[i], tprs[i])\n",
        "    interp_tpr[0] = 0.0\n",
        "    interp_tprs.append(interp_tpr)\n",
        "\n",
        "  mean_tpr = np.mean(interp_tprs, axis=0)\n",
        "  mean_tpr[-1] = 1.0\n",
        "\n",
        "  # Compute mean & standard deviation of the AUC.\n",
        "  mean_auc = metrics.auc(mean_fpr, mean_tpr)\n",
        "  std_auc = np.std(results['roc_auc'])\n",
        "\n",
        "  lw = 2\n",
        "  ax.plot(\n",
        "      mean_fpr,\n",
        "      mean_tpr,\n",
        "      color=\"darkorange\",\n",
        "      label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n",
        "      lw=lw,\n",
        "  )\n",
        "\n",
        "  std_tpr = np.std(interp_tprs, axis=0)\n",
        "  tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "  tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "\n",
        "  ax.fill_between(\n",
        "      mean_fpr,\n",
        "      tprs_lower,\n",
        "      tprs_upper,\n",
        "      color=\"grey\",\n",
        "      alpha=0.2,\n",
        "      label=r\"$\\pm$ 1 std. dev.\",\n",
        "  )\n",
        "\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  ax.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
        "  ax.set_xlim([0.0, 1.0])\n",
        "  ax.set_ylim([0.0, 1.0])\n",
        "  plt.xlabel(\"False Positive Rate\")\n",
        "  plt.ylabel(\"True Positive Rate\")\n",
        "  plt.title(\"Receiver operating characteristic for %s\" %algorithm_name)\n",
        "  plt.show()\n",
        "\n",
        "plot_auc_xfold(results, learning_algorithm.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCQOKoC8SJON"
      },
      "source": [
        "Since we're training on 80% of 80% (64%) 5 times, we aren't surprised that the average AUC is slightly lower than when we trained on the entire test set. Maybe we can do a better job with a Deep Learning, nonlinear classifier!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y7dH8nsU_0K"
      },
      "source": [
        "##Train and Evaluate a Dense Neural Network Classifier\n",
        "\n",
        "Let’s define a **Dense Neural Network (DNN)** classifier. Unlike Logistic Regression, a DNN is a nonlinear classifier. We’ll use TensorFlow’s Keras API to define a simple classifier and wrap it inside an extension class to BaseLearningAlgorithm defined above. TensorFlow is a lower-level library that exposes a Keras-compatible programming interface. You can use either the [Keras library](https://keras.io/), or the [TensorFlow Keras API](https://www.tensorflow.org/guide/keras).\n",
        "\n",
        "An incomplete implementation of `DenseNNLearningAlgorithm` is provided below to get you started, but also provide you with some initial experience in creating a neural network using the Keras API. Don’t worry too much about some unfamiliar concepts like *binary cross-entropy loss*, *ReLU activation*, *dropout layers*, etc. We’ll cover them in greater detail during the class. You should become familiar with How Sequential models are define, and how to assemble a multi-layered model using Dense and Dropout layers.\n",
        "\n",
        "Also, neural networks tend to be sensitive to features with different ranges that can lead to vanishing or exploding gradients, which we’ll explore later in greater detail. To minimize the danger, it’s advisable to normalize the data, by shifting by the mean and dividing by the standard deviation, using the following equation $$\\hat{x}_{d} = \\frac{x_d-\\mu_d}{\\sigma_d} $$\n",
        "\n",
        "where:\n",
        "*  $\\hat{x}_d$: Normalized feature indexed by $d$\n",
        "*  $x_d$: Native (original) feature indexed by $d$\n",
        "*  $\\mu_d$: Feature mean indexed by $d$\n",
        "*  $\\sigma_d$: Feature standard deviation indexed by $d$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Of course, you’ll want to use exactly the same $\\mu$ and $\\sigma$ in training and in test (i.e., calculate/use in the `train()`, and use it in `predict()`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4Oqna53Snbe"
      },
      "outputs": [],
      "source": [
        "#@title Train and Evaluate a Dense Neural Network Classifier\n",
        "\n",
        "class DenseNNLearningAlgorithm(BaseLearningAlgorithm):\n",
        "  \"\"\"Base class for a Supervised Learning Algorithm.\"\"\"\n",
        "\n",
        "  def __init__(self, input_dim: int, dropout: float, layer_width: int,\n",
        "                n_hidden_layers: int, epochs: int, steps_per_epoch: int,\n",
        "                batch_size: int, optimizer: str):\n",
        "    self._epochs = epochs\n",
        "    self._steps_per_epoch = steps_per_epoch\n",
        "    self._batch_size = batch_size\n",
        "    self._model = self._get_model(input_dim,  dropout, layer_width,\n",
        "                 n_hidden_layers, optimizer)\n",
        "\n",
        "\n",
        "  def _get_model(self, input_dim: int, dropout: float, layer_width: int,\n",
        "                 n_hidden_layers: int, optimizer: str) -> tf.keras.Sequential:\n",
        "    \"\"\"Creates a Keras Neural Network model for Classification.\n",
        "    Creates a simple stack of dense/dropout layers with equal width.\n",
        "    Args:\n",
        "      input_dim: width of the input layer\n",
        "      dropout: dropout probability for each hidden layer\n",
        "      layer_width: hidden layer width\n",
        "      n_hidden_layers: number of hidden layers\n",
        "      optimizer: Name of the optimizer in use\n",
        "    Returns:\n",
        "      tf.keras.Sequential model.\n",
        "    \"\"\"\n",
        "    # Define the a Sequential Keras model.\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # Add a Dense layer of input_dim, and layer_width, select ReLu activation,\n",
        "    # and add one Dropout layer.\n",
        "    model.add(\n",
        "        tf.keras.layers.Dense(\n",
        "            layer_width, input_dim=input_dim, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(dropout))\n",
        "\n",
        "    # Next, add a Dense of layer_width, and ReLu activation, and Dropout layer\n",
        "    # for each hidden_layer in the loop.\n",
        "    for _ in range(n_hidden_layers):\n",
        "      model.add(tf.keras.layers.Dense(layer_width, activation='relu'))\n",
        "      model.add(tf.keras.layers.Dropout(dropout))\n",
        "\n",
        "    # Add a Dense output layer with a single output with a\n",
        "    # sigmoid activaton to squees the outputs into a probability-like range.\n",
        "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Now compile the model, by specifying a binary cross-entropy loss,\n",
        "    # and setting the optimizer from the input arguments.\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer=optimizer,\n",
        "        metrics=[tf.keras.metrics.binary_accuracy])\n",
        "    return model\n",
        "\n",
        "\n",
        "  def train(self, X_train:np.array, y_train: np.array) -> None:\n",
        "    \"\"\"Trains a model from labels y and examples X.\"\"\"\n",
        "\n",
        "    # Compute the means and standard deviations for each feature.\n",
        "    # Hint: make these class variables so you can use them in testing.\n",
        "    self._st_devs = np.std(X_train, axis = 0)\n",
        "    self._means = np.mean(X_train, axis = 0)\n",
        "\n",
        "    # Now normlize the training data.\n",
        "    X_train_normalized  = (X_train - self._means)/self._st_devs\n",
        "\n",
        "    # Create datasets for both examples and labels\n",
        "    dx = tf.data.Dataset.from_tensor_slices(X_train_normalized)\n",
        "    dy = tf.data.Dataset.from_tensor_slices(y_train)\n",
        "\n",
        "    # Zip the two datasets together into a [X|y] training dataset.\n",
        "    train_dataset = tf.data.Dataset.zip(\n",
        "        (dx, dy)).shuffle(50).repeat().batch(self._batch_size)\n",
        "\n",
        "    # Similar to LogisticRegresssion, invoke the fit() method.\n",
        "    self._model.fit(\n",
        "        x=train_dataset,\n",
        "        steps_per_epoch=self._steps_per_epoch,\n",
        "        verbose=0,\n",
        "        epochs=self._epochs,\n",
        "        )\n",
        "\n",
        "  def predict(self, X_test: np.array) -> np.array:\n",
        "    \"\"\"Predicts on an unlabeled sample, X.\"\"\"\n",
        "\n",
        "    # Normalize the test data using the constants computer earlier.\n",
        "    X_test_normalized  = (X_test - self._means)/self._st_devs\n",
        "    # Then invoke the predict method.\n",
        "    return self._model.predict(X_test_normalized, verbose=1, steps=1)\n",
        "\n",
        "  @property\n",
        "  def name(self) -> str:\n",
        "    return \"Dense NN\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf-7-A3chYCC"
      },
      "source": [
        "Let's invoke the DNNmodel. The handy Keras plotting utility can display the network architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "EjZOrtiwzas-"
      },
      "outputs": [],
      "source": [
        "#@title Instantiate and display the DNN CLassifier\n",
        "\n",
        "learning_algorithm =  DenseNNLearningAlgorithm( input_dim = 24, dropout = 0.1, layer_width = 16,\n",
        "                 n_hidden_layers = 1, epochs = 25, steps_per_epoch = 8, batch_size = 64, optimizer = 'sgd')\n",
        "tf.keras.utils.plot_model(learning_algorithm._model, show_shapes=True, rankdir=\"LR\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg6KrxhpOY55"
      },
      "source": [
        "## Question 5.1\n",
        "How many parameters are in this model compared to the number of parameters in the Logistic Regression model?\n",
        "\n",
        "## Answer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lMt64WJhzXz"
      },
      "source": [
        "Now we can execute cross-fold train/eval and chart the ROC curve and AUC on the same data like with Logistic Regression, just with the DNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWJYppYra4V_"
      },
      "outputs": [],
      "source": [
        "results = train_eval_xval(X_train, y_train, K_FOLDS, learning_algorithm)\n",
        "print(\"%s average auc = %3.3f\" %(learning_algorithm.name, np.mean(results['roc_auc'])))\n",
        "plot_auc_xfold(results, learning_algorithm.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm1kIkYvfK-y"
      },
      "source": [
        "##Question 5.2\n",
        "Compare the AUC scores and the ROC charts from DNN and Logistic Regression. Is one model approach better than the other? Explain the differences or similarities in the performances of both learning algorithms.\n",
        "\n",
        "##Answer\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoM2_vhyheJO"
      },
      "source": [
        "# 6 Hyperparameter Tuning the DNN Classifier with hparams\n",
        "\n",
        "We can speculate that the original hyperparameters weren't ideal that lead to\n",
        "less impressive results. We would like an automatic way of searching through the hyperparameter space.\n",
        "\n",
        "\n",
        "Let's see if we can find better parameters for the Dense NN. First, we have to choose the parameters we'd like to investigate:\n",
        "\n",
        "\n",
        "*   Dropout (Probability that a node will be dropped in training.)\n",
        "*   Layer Width (i.e., number of nodes in the layer)\n",
        "*   Number of hidden layers\n",
        "*   The optimizer: RMS Prop, SGD, and Adam\n",
        "\n",
        "Then we need to specify the possible intervals and ranges. For example, we may not want to try 1, 2,..., 999, 1000 layers. Instead we might find it more efficient to try powers of 2 to save some time.\n",
        "\n",
        "We'll search through the hyperparameter space using grid search, which works well for small models and limited search spaces. Other algorithms exist that are more efficient (but less complete) in searching through the hyperparameter space.\n",
        "\n",
        "We'll be using [TensorBoard](https://www.tensorflow.org/tensorboard), TensorFlow's visualization toolkit to gain some insights into performance from the different hyperparameter settings. Specifically, the HParams view will be useful in tuning our algorithm.\n",
        "\n",
        "Additional details are provided in [Hyperparameter Tuning with the HParams Dashboard](https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams), which was largely adapted here to our classification problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVQvzfSLnHyC"
      },
      "source": [
        "First, loadup TensorBoard and clear out any old data if necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8p3Tbx8cWEFA"
      },
      "outputs": [],
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEWCCQYkWIdA"
      },
      "outputs": [],
      "source": [
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sy0q35TnnGfc"
      },
      "source": [
        "Next define the hyperparameter ranges using` hp.HParam`, `hp.RealInterval`, and `hp.Discrete`. A good set of ranges to explot might be:\n",
        "\n",
        "*  `dropout`: {0.0, 0.05, ..., 0.2},\n",
        "*  `num_units`: {8, 16, 32, 64},\n",
        "*  `num_layers`: {1, 2, 3},\n",
        "*  `optimizer`: {`adam`, `sgd`, and `rmsprop`}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Euw0agpWb4V"
      },
      "outputs": [],
      "source": [
        "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.00, 0.20))\n",
        "# TODO: define multiple discrete steps [4, 8, 12, .., 48] for layer width and call it HP_LAYER_WIDTH\n",
        "HP_NUM_LAYERS = hp.HParam('num_layers', hp.Discrete([1,2,3]))\n",
        "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd', 'rmsprop']))\n",
        "\n",
        "METRIC_AUC = 'auc'\n",
        "# Set up the hparams configuration and write file.\n",
        "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
        "  hp.hparams_config(\n",
        "    hparams=[HP_DROPOUT, HP_LAYER_WIDTH, HP_NUM_LAYERS, HP_OPTIMIZER],\n",
        "    metrics=[hp.Metric(METRIC_AUC, display_name='AUC')],\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byG0Ekojo5N7"
      },
      "source": [
        "For each run, log an hparams summary with the hyperparameters and AUC:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ofx878qvkAFW"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_test_model(hparams):\n",
        "\n",
        "  learning_algorithm =  DenseNNLearningAlgorithm( input_dim = 24, dropout = hparams[HP_DROPOUT], layer_width = hparams[HP_LAYER_WIDTH],\n",
        "                 n_hidden_layers = hparams[HP_NUM_LAYERS], epochs = 25, steps_per_epoch = 8, batch_size = 64, optimizer =  hparams[HP_OPTIMIZER])\n",
        "  results = train_eval_xval(X_train, y_train, K_FOLDS, learning_algorithm)\n",
        "  mean_auc = np.mean(results['roc_auc'])\n",
        "  print(\"%s average auc = %3.3f, min auc = %3.3f, max auc = %3.3f\" %(learning_algorithm.name, mean_auc,  np.min(results['roc_auc']),  np.max(results['roc_auc']) ))\n",
        "  return  np.mean(results['roc_auc'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8j-fO6nEXRfW"
      },
      "outputs": [],
      "source": [
        "def run(run_dir, hparams, step):\n",
        "  with tf.summary.create_file_writer(run_dir).as_default():\n",
        "    hp.hparams(hparams)  # record the values used in this trial\n",
        "    mean_auc = train_test_model(hparams)\n",
        "    tf.summary.scalar(METRIC_AUC, mean_auc, step=step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDSFtWBepIAp"
      },
      "source": [
        "You can now try multiple experiments, training each one with a different set of hyperparameters.\n",
        "\n",
        "For simplicity, use a grid search: try all combinations of the discrete parameters and just the lower and upper bounds of the real-valued parameter. For more complex scenarios, it might be more effective to choose each hyperparameter value randomly (this is called a random search). There are more advanced methods that can be used.\n",
        "\n",
        "Feel free to enjoy a coffee or tea at this point, and let the computer work for you. ☕\n",
        "\n",
        "\n",
        "## Question 6.1\n",
        "What are the advantages and disadvantages of grid search? What alternative search approaches might you consider if it's not feasible to perform a robust grid search?\n",
        "\n",
        "## Answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbqT5n-AXd0h"
      },
      "outputs": [],
      "source": [
        "session_num = 0\n",
        "hp_step = 0.05\n",
        "dropout_rate = 0\n",
        "for num_units in HP_LAYER_WIDTH.domain.values:\n",
        "  for dropout_rate in np.arange(HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value, hp_step):\n",
        "    for num_layers in HP_NUM_LAYERS.domain.values:\n",
        "      for optimizer in HP_OPTIMIZER.domain.values:\n",
        "        hparams = {\n",
        "            HP_LAYER_WIDTH : num_units,\n",
        "            HP_DROPOUT: dropout_rate,\n",
        "            HP_NUM_LAYERS: num_layers,\n",
        "            HP_OPTIMIZER: optimizer,\n",
        "        }\n",
        "        run_name = \"run-%d\" % session_num\n",
        "        print('--- Starting trial: %s' % run_name)\n",
        "        print({h.name: hparams[h] for h in hparams})\n",
        "        run('logs/hparam_tuning/' + run_name, hparams, session_num)\n",
        "        session_num += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBHiV_yHqbqx"
      },
      "source": [
        "The left pane of the dashboard provides filtering capabilities that are active across all the views in the HParams dashboard:\n",
        "\n",
        "*  Filter which hyperparameters/metrics are shown in the dashboard\n",
        "*   Filter which hyperparameter/metrics values are shown in the dashboard\n",
        "*   Filter on run status (running, success, ...)\n",
        "* Sort by hyperparameter/metric in the table view\n",
        "Number of session groups to show (useful for performance when there are many experiments)\n",
        "\n",
        "The HParams dashboard has three different views, with various useful information:\n",
        "\n",
        "*  The Table View lists the runs, their hyperparameters, and their metrics.\n",
        "*  The Parallel Coordinates View shows each run as a line going through an axis for each hyperparemeter and metric. Click and drag the mouse on any axis to mark a region which will highlight only the runs that pass through it. This can be useful for identifying which groups of hyperparameters are most important. *  The axes themselves can be re-ordered by dragging them.\n",
        "* The Scatter Plot View shows plots comparing each hyperparameter/metric with each metric. This can help identify correlations. Click and drag to select a region in a specific plot and highlight those sessions across the other plots.\n",
        "\n",
        "A table row, a parallel coordinates line, and a scatter plot market can be clicked to see a plot of the metrics as a function of training steps for that session (although in this tutorial only one step is used for each run).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xf4KM-U2bbP_"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs/hparam_tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i748SePvJ2Du"
      },
      "source": [
        "## Question 6.2\n",
        "Describe the results of hyperparameter tuning. Were you able to find a set of hyperparameters that yielded better performance than Logistic Regression? What was the best AUC score you were able to achieve? What hyperparameter settings made the largest contribution?\n",
        "\n",
        "##Answer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrCiGBAytUIK"
      },
      "source": [
        "# 7 Comparing Logistic Regression against DNN Classifiers\n",
        "\n",
        "Next, choose the ebst hyperparameter values and execute train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "0mqAHl9vt2vQ"
      },
      "outputs": [],
      "source": [
        "#@title Instantiate and display the DNN Classifier\n",
        "dropout = 0.0\n",
        "num_layers =3\n",
        "num_units =48\n",
        "optimizer = 'adam'\n",
        "learning_algorithm =  DenseNNLearningAlgorithm( input_dim = 24, dropout = 0.0, layer_width = num_units,\n",
        "                 n_hidden_layers = num_layers, epochs =25, steps_per_epoch = 8, batch_size = 64,  optimizer = optimizer)\n",
        "\n",
        "\n",
        "learning_algorithm.train(X_train, y_train)\n",
        "\n",
        "# Using the test set get the predictions.\n",
        "predictions = learning_algorithm.predict(X_val)\n",
        "\n",
        "# Compute the false positive and true positive rates.\n",
        "fpr, tpr, _ = metrics.roc_curve(y_val, predictions)\n",
        "roc_auc = metrics.auc(fpr, tpr)\n",
        "print('The ROC-AUC is %3.3f' %roc_auc)\n",
        "plot_auc(fpr, tpr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUscojUVrQAb"
      },
      "source": [
        "##Question 7.1\n",
        "Compare and contrast linear models with nonlinear classifier models. What are the advantages and disadvantages of either?\n",
        "\n",
        "## Answer\n",
        "\n",
        "\n",
        "## Question 7.2\n",
        "When you train a new model using the hyperparameters from the best run, are you able to match or exceed logistic regression?\n",
        "\n",
        "## Answer\n",
        "\n",
        "\n",
        "## Question 7.3\n",
        "You should always get the same result when executing Logistic Regression ifthe data set is fixed. However, when running the DNN multiple times, you'll get different results. Can you provide an explanation about what makes the DNN produce different results, even when being trained on the same data?\n",
        "\n",
        "## Answer\n",
        "\n",
        "\n",
        "## Question 7.4\n",
        "Given this experiment, when do you think it's appropriate to use linear instead nonlinear methods, like DNNs?\n",
        "\n",
        "## Answer"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
