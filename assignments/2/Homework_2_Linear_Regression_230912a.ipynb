{"cells":[{"cell_type":"markdown","metadata":{"id":"ris4FZxnH9iF"},"source":["# Homework 2 CSCI 4364/6364 Machine Learning\n","\n","##**Adventures in Linear Regression**\n","v.20230912a\n","\n","**Due Date: 9/26/2023, 23:59 ET**\n","\n","---\n","\n","**Purpose:**\n","This homework will familiarize you with linear regression using the [Prostate Cancer dataset](https://hastie.su.domains/ElemStatLearn/data.html). First, you’ll work with the least squares. Then you’ll investigate regression using L2 (Ridge) and L1 (Lasso) regularization. Finally, you will implement an iterative version of L2 Regularization using gradient descent.\n","\n","**Note**: Besides part 3, you should implement your solution with the fundamental equations we discussed in class and in Hastie, chapter 3. *Only in part 3, you should use Scikit-Learn*.\n","\n","\n","\n","---\n","**Submission Instructions:**\n","This assignment will be done entirely in this Colaboratory notebook, and you will submit your notebook via GWU blackboard. Please embed your code in code blocks and add in comments into the text blocks.\n","\n","**Important:** Please submit your assignment with outputs, by navigating to Edit >> Notebook Settings and ensuring *Omit code cell output when saving this notebook* is **NOT** selected.\n","\n","---\n","\n","**Grading on the notebook:**\n","\n","Parts 1 - 4 of this notebook are worth 5% of the semester grade, where 3% is completion and full functionality, and 2% is based on comments and descriptions, and well-written and commented Python code, based on the coding standards. The notebook should be fully explained and work in its entirety when you submit it.\n","\n","**Extra Credit!** Besides being a great learning experience about convex optimization, part 5 is **optional**, but worth up to 2% of the semester grade.\n","\n","**Coding Standards:**\n","Throughout this course, we will use Google’s Python Style Guide (https://google.github.io/styleguide/pyguide.html) as the coding standard for homework and project submission. A big part of machine learning in industry is applying good programming practices.\n"]},{"cell_type":"markdown","metadata":{"id":"z2iRXG9B5hPK"},"source":["**Name:** [please add your name here]\n","\n","**GW ID:** [your GWU student ID]"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"DnXm2ekDUw3P"},"outputs":[],"source":["#@title Imports\n","##########################################################\n","# Always include all imports at the first executable cell.\n","##########################################################\n","from abc import ABC, abstractmethod # Abstract Base Classes for Python\n","import pandas as pd # Pandas dataframe libaries\n","import numpy as np # Numpy numerical computation library\n","import seaborn as sns\n","from sklearn.linear_model import Lasso\n","from sklearn import metrics # Used to compute metrics, such as the Area Under the Curve (AUC)\n","import matplotlib.pyplot as plt # Plotting library.\n","from typing import List, Tuple, Mapping # Common types for Python type definitions\n"]},{"cell_type":"markdown","metadata":{"id":"pO4burfLcwLC"},"source":["# Data Preperation & Feature Analysis (Prostate Cancer)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6b-2cRhwTybF"},"outputs":[],"source":["#@title Load the Prostate Cancer dataset\n","_SEED = 1223\n","random_state = np.random.RandomState(_SEED)\n","df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/prostate.csv')\n","# Randomize the rows\n","df = df.sample(frac =  1, random_state=random_state)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BYAUOdwhUwhh"},"outputs":[],"source":["#@title Split into training and test set\n","# Following Hastie p.50, we create a training set of 67\n","split_index = 67\n","df_train = df.iloc[:split_index]\n","df_test = df.iloc[split_index:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Wkl8unYc-S7"},"outputs":[],"source":["#@title Display pair plots\n","sns.pairplot(df, kind=\"reg\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DGTFmioJY4gx"},"outputs":[],"source":["#@title Split the labels and convert to numpy arrays\n","y_train = df_train['lpsa'].to_numpy()\n","X_train = df_train.copy().drop(columns = ['lpsa']).to_numpy()\n","y_test = df_test['lpsa'].to_numpy()\n","X_test = df_test.copy().drop(columns = ['lpsa']).to_numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L7xSPWoLQWYq"},"outputs":[],"source":["class BaseLearningAlgorithm(ABC):\n","  \"\"\"Base class for a Supervised Learning Algorithm.\"\"\"\n","\n","  @abstractmethod\n","  def train(self, X_train:np.array, y_train: np.array) -> None:\n","    \"\"\"Trains a model from labels y and examples X.\"\"\"\n","\n","  @abstractmethod\n","  def predict(self, X_test: np.array) -> np.array:\n","    \"\"\"Predicts on an unlabeled sample, X.\"\"\"\n","\n","  @property\n","  @abstractmethod\n","  def name(self) -> str:\n","    \"\"\"Returns the name of the algorithm.\"\"\""]},{"cell_type":"markdown","metadata":{"id":"7_klHAmDNAyX"},"source":["# 1. Linear Regression with Least Squares\n","Implement a class called `BasicLeastSquaresRegression` that extends `BaseLearningAlgorithm` with “vanilla” least squares regression described in Hastie 3.2. Compute the $\\boldsymbol{\\beta}$ coefficient vector and solve for $\\hat{y} $ and compute the performance result as the mean squared error on the test set. Use only numpy for your solution.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"s2193UPuQ2qb"},"source":["**Questions:**\n","\n","**1.1** What MSE loss score do you get with Least Squares?\n","\n","**1.2** What variables carry the greatest influence (i.e., are the most important) in the least squares regression?\n","\n","**1.3** Does normalizing the data improve performance (lower MSE loss)? If so, why?\n"]},{"cell_type":"markdown","metadata":{"id":"27dtemGJd6qq"},"source":["# 2. Linear Regression with L2 Regularization (Ridge)\n","\n","Using the closed-form solution to Ridge Regression, implement a class called `RidgeRegression` that extends `BaseLearningAlgorithm` based on Hastie 3.41. Iterate through the regression penalty term, $\\lambda$, and plot (a) MSE loss as a function of $\\lambda$, and (b) each coefficient weight as a function of $\\lambda$. Use only numpy and matplotlib for your solution.\n"]},{"cell_type":"markdown","metadata":{"id":"MmB1Zg4_RenB"},"source":["**Questions:**\n","\n","\n","**2.1** What $\\lambda$ do you get the best performance (i.e., lowest MSE)?\n","\n","**2.2** Compare the results from part 1, when $\\lambda = 0$. Why are the results similar or different?\n","\n","**2.3** Compare the weights with the weights from part 1? If you were to rank descending by the absolute value of the weights, how different is the ordering with part 1? Is the most important variable in part 1 the same as in part 2? If not, can you provide a reason?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Xy-PAEjbRypK"},"source":["# 3. Linear Regression with L1 Regularization (Lasso)\n","\n","Unlike Ridge Regression, there is no closed-form solution for Lasso, meaning there is no normal equation we can solve to immediately get all of our ideal model parameters. While it can be solved by minimizing one coordinate a time using a technique called [Coordinate Descent](http://www.adeveloperdiary.com/data-science/machine-learning/introduction-to-coordinate-descent-using-least-squares-regression/), here you can implement your solution using Scikit-Learn Lasso inside a class called `LassoRegression` which also extends `BaseLearningAlgorithm`. Like part 2, iterate through the regression penalty term, $\\alpha$ , and plot (a) MSE loss as a function of $\\alpha$, and (b) each coefficient weight as a function of $\\alpha$.\n","\n","**Note**: here we swap notation a little, replacing $\\lambda$ with $\\alpha$ to fit with the Scikit-Learn convention.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zza9NC3LSK46"},"source":["**Questions:**\n","\n","**3.1** Under what conditions would you prefer L2 or L1 regression over vice versa? (Consider the discussion in Hastie 3.6.)\n","\n","**3.2** Why do some coefficients become zero? Do you think this may be a method of subset selection as described in Hastie 3.3?\n","\n","**3.3** Which method performs better (i.e., has the lower MSE)?\n","\n","**3.4** Comparing the relative ranking of the weights at the lowest MSE, do Ridge Regression and Lasso Regression “agree” on the most important weights?\n","\n","**3.5** In your own words, define **bias** and **variance**. Describe how bias and variance affected the results of parts 1, 2, and 3.\n"]},{"cell_type":"markdown","metadata":{"id":"1m_aTGuhSYFr"},"source":["# 4.  Iterative Optimization with Gradient Descent\n","Implement a class called `RidgeRegressionGradDescent` that extends `BaseLearningAlgorithm` that performs linear regression with L2 regularization using [gradient descent](https://www.deeplearningbook.org/contents/numerical.html), Goodfellow 4.5. Plot both training and test MSE loss result vs. iteration.  Iterate through the regression penalty term, $\\lambda$ , and plot (a) MSE loss as a function of $\\lambda$, and (b) each coefficient weight as a function of $\\lambda$. Use only numpy and matplotlib for your solution.\n"]},{"cell_type":"markdown","metadata":{"id":"mYiC_VJ_Ss8d"},"source":["**Questions:**\n","\n","**4.1** How many iterations are required to converge, and are there any performance differences compared to part 2?\n","\n","**4.2** Derive the gradient of L2 regularization loss. Encode your answer as [$\\LaTeX$ equations](https://colab.research.google.com/github/bebi103a/bebi103a.github.io/blob/master/lessons/00/intro_to_latex.ipynb), and justify every step, please.\n","\n","**4.3** Do you converge on the same minimum loss value? How do the coefficients compare to part 1?\n"]},{"cell_type":"markdown","metadata":{"id":"kcKViVOgXbBU"},"source":["#5. [OPTIONAL] Iterative Optimization using Coordinate Descent\n","\n","**Worth up to 2% of the semester grade.**\n","\n","Review Hastie 3.8.6 and this [presentation](https://www.cs.cmu.edu/~pradeepr/convexopt/Lecture_Slides/coordinate_descent.pdf) on coordinate descent. Using just numpy reimplement part 3 with coordinate descent. Like part 3, iterate through the regression penalty term, $\\lambda$, and plot (a) MSE loss as a function of $\\lambda$, and (b) each coefficient weight as a function of $\\lambda$. Use only numpy and matplotlib for your solution.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wEGQVz2AX3b7"},"source":["**Questions:**\n","\n","**5.1** How does your implementation compare with Scikit-Learn’s implementation in terms of minimum MSE loss and coefficients?\n","\n","**5.2** Is the optimization surface convex or non-convex?\n","\n","**5.3** What makes the optimization surface *non-smooth* and how is coordinate descent able to overcome this problem?\n"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":0}
